{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-12-01T09:39:45.963355Z",
     "end_time": "2023-12-01T09:39:55.656971Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gxb18167\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1\n",
      "GPU Available: False\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from config import get_config#\n",
    "import generate_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from transformers import BertLMHeadModel, BartTokenizer\n",
    "from data import ZuCo_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-01T09:39:55.656971Z",
     "end_time": "2023-12-01T09:39:59.517376Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "task_name = \"task1, task2, taskNRv2\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-01T09:39:59.533025Z",
     "end_time": "2023-12-01T09:39:59.595531Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "''' set up dataloader '''\n",
    "\n",
    "whole_dataset_dicts = []\n",
    "\n",
    "if 'task1' in task_name:\n",
    "    dataset_path_task1 = r'I:\\Science\\CIS-YASHMOSH\\niallmcguire\\ZuCo\\task1-SR\\pickle\\task1-SR-dataset.pickle'\n",
    "    with open(dataset_path_task1, 'rb') as handle:\n",
    "        whole_dataset_dicts.append(pickle.load(handle))\n",
    "\n",
    "if 'task2' in task_name:\n",
    "    dataset_path_task2 = r'I:\\Science\\CIS-YASHMOSH\\niallmcguire\\ZuCo\\task2-NR\\pickle\\task2-NR-dataset.pickle'\n",
    "    with open(dataset_path_task2, 'rb') as handle:\n",
    "        whole_dataset_dicts.append(pickle.load(handle))\n",
    "\n",
    "if 'task3' in task_name:\n",
    "    dataset_path_task3 = r'I:\\Science\\CIS-YASHMOSH\\niallmcguire\\ZuCo\\task3-TSR\\pickle\\task3-TSR-dataset.pickle'\n",
    "    with open(dataset_path_task3, 'rb') as handle:\n",
    "        whole_dataset_dicts.append(pickle.load(handle))\n",
    "\n",
    "if 'taskNRv2' in task_name:\n",
    "    dataset_path_taskNRv2 = r'I:\\Science\\CIS-YASHMOSH\\niallmcguire\\ZuCo\\task2-NR-2.0\\pickle\\task2-NR-2.0-dataset.pickle'\n",
    "    with open(dataset_path_taskNRv2, 'rb') as handle:\n",
    "        whole_dataset_dicts.append(pickle.load(handle))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-01T09:39:59.595531Z",
     "end_time": "2023-12-01T09:45:01.622973Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded in 3 task datasets\n"
     ]
    }
   ],
   "source": [
    "print(\"Loaded in\", len(whole_dataset_dicts), \"task datasets\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-01T09:45:01.612962Z",
     "end_time": "2023-12-01T09:45:01.634878Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "Task_Dataset_List = whole_dataset_dicts\n",
    "if not isinstance(whole_dataset_dicts,list):\n",
    "    Task_Dataset_List = [whole_dataset_dicts]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-01T09:45:01.634878Z",
     "end_time": "2023-12-01T09:45:01.651848Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of word embeddings: 5860\n",
      "[INFO]loading 3 task datasets\n",
      "[INFO]using subjects:  ['ZAB', 'ZDM', 'ZDN', 'ZGW', 'ZJM', 'ZJN', 'ZJS', 'ZKB', 'ZKH', 'ZMG', 'ZPH']\n",
      "augmentation size = 40\n",
      "train divider = 320\n",
      "dev divider = 360\n",
      "[INFO]initializing a train set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gxb18167\\Anaconda3\\lib\\site-packages\\torch\\_tensor.py:775: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discard length zero instance:  Weiss and Speck never make a convincing case for the relevance of these two 20th-century footnotes.\n",
      "discard length zero instance:  Reassuring, retro uplifter.\n",
      "discard length zero instance:  Flaccid drama and exasperatingly slow journey.\n",
      "++ adding task to dataset, now we have: 3330\n",
      "[INFO] Augmentation Counter =  40\n",
      "[INFO]using subjects:  ['ZAB', 'ZDM', 'ZDN', 'ZGW', 'ZJM', 'ZJN', 'ZJS', 'ZKB', 'ZKH', 'ZKW', 'ZMG', 'ZPH']\n",
      "augmentation size = 30\n",
      "train divider = 240\n",
      "dev divider = 270\n",
      "[INFO]initializing a train set...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m BartTokenizer\u001B[38;5;241m.\u001B[39mfrom_pretrained(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfacebook/bart-large\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m----> 2\u001B[0m train_set \u001B[38;5;241m=\u001B[39m \u001B[43mZuCo_dataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mwhole_dataset_dicts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtrain\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msubject\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mALL\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43meeg_type\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mGD\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbands\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m_t1\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m_t2\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m_a1\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m_a2\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m_b1\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m_b2\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m_g1\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m_g2\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msetting\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43munique_sent\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_add_CLS_token\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\EEG-To-Text\\SIGIR_Development\\EEG-GAN\\data.py:296\u001B[0m, in \u001B[0;36mZuCo_dataset.__init__\u001B[1;34m(self, input_dataset_dicts, phase, tokenizer, subject, eeg_type, bands, setting, is_add_CLS_token)\u001B[0m\n\u001B[0;32m    293\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minputs\u001B[38;5;241m.\u001B[39mappend(input_sample)\n\u001B[0;32m    294\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m augmentation_counter \u001B[38;5;241m<\u001B[39m augmentation_size:\n\u001B[1;32m--> 296\u001B[0m     input_sample_synthetic \u001B[38;5;241m=\u001B[39m \u001B[43mgenerate_samples\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate_synthetic_samples\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_sample\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgen_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mword_embeddings\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mEEG_word_level_embeddings\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    297\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m input_sample_synthetic \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    298\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minputs\u001B[38;5;241m.\u001B[39mappend(input_sample_synthetic)\n",
      "File \u001B[1;32m~\\PycharmProjects\\EEG-To-Text\\SIGIR_Development\\EEG-GAN\\generate_samples.py:147\u001B[0m, in \u001B[0;36mgenerate_synthetic_samples\u001B[1;34m(input_sample, gen_model, word_embeddings, EEG_word_level_embeddings)\u001B[0m\n\u001B[0;32m    144\u001B[0m word_embedding_tensor \u001B[38;5;241m=\u001B[39m word_embedding_tensor\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m    146\u001B[0m g_output \u001B[38;5;241m=\u001B[39m generate_samples(gen_model, input_z, word_embedding_tensor)\n\u001B[1;32m--> 147\u001B[0m EEG_synthetic_denormalized \u001B[38;5;241m=\u001B[39m (g_output \u001B[38;5;241m*\u001B[39m np\u001B[38;5;241m.\u001B[39mmax(\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mabs\u001B[49m\u001B[43m(\u001B[49m\u001B[43mEEG_word_level_embeddings\u001B[49m\u001B[43m)\u001B[49m)) \u001B[38;5;241m+\u001B[39m np\u001B[38;5;241m.\u001B[39mmean(\n\u001B[0;32m    148\u001B[0m     EEG_word_level_embeddings)\n\u001B[0;32m    150\u001B[0m synthetic_sample \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(EEG_synthetic_denormalized[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m], dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mfloat)\n\u001B[0;32m    152\u001B[0m synthetic_sample \u001B[38;5;241m=\u001B[39m synthetic_sample\u001B[38;5;241m.\u001B[39mresize(\u001B[38;5;241m840\u001B[39m)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
    "train_set = ZuCo_dataset(whole_dataset_dicts, 'train', tokenizer, subject = 'ALL', eeg_type = 'GD', bands = ['_t1','_t2','_a1','_a2','_b1','_b2','_g1','_g2'], setting = 'unique_sent', is_add_CLS_token = False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-30T15:23:04.117118Z",
     "end_time": "2023-11-30T15:23:39.817158Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(train_set)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-30T15:23:39.848233Z",
     "end_time": "2023-11-30T15:23:40.009521Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-30T15:23:53.530297Z",
     "end_time": "2023-11-30T15:23:53.604522Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-30T15:23:53.572253Z",
     "end_time": "2023-11-30T15:23:53.604892Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-30T15:23:53.641862Z",
     "end_time": "2023-11-30T15:23:53.677276Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-30T15:23:53.858272Z",
     "end_time": "2023-11-30T15:23:53.868503Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-30T15:23:54.018929Z",
     "end_time": "2023-11-30T15:23:54.054677Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-30T15:23:54.202729Z",
     "end_time": "2023-11-30T15:23:54.225681Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-30T15:23:54.382282Z",
     "end_time": "2023-11-30T15:23:54.420453Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-30T15:23:54.514391Z",
     "end_time": "2023-11-30T15:23:54.553274Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-30T15:23:54.674014Z",
     "end_time": "2023-11-30T15:23:54.687744Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_eeg_word_embedding(word, eeg_type = 'GD', bands = ['_t1','_t2','_a1','_a2','_b1','_b2','_g1','_g2']):\n",
    "    EEG_frequency_features = []\n",
    "    EEG_word_level_label = word['content']\n",
    "    for band in bands:\n",
    "        EEG_frequency_features.append(word['word_level_EEG'][eeg_type][eeg_type+band])\n",
    "    word_eeg_embedding = np.concatenate(EEG_frequency_features)\n",
    "    if len(word_eeg_embedding) != 105*len(bands):\n",
    "        print(f'expect word eeg embedding dim to be {105*len(bands)}, but got {len(word_eeg_embedding)}, return None')\n",
    "        word_eeg_embedding = None\n",
    "    else:\n",
    "        word_eeg_embedding = word_eeg_embedding.reshape(105, 8)\n",
    "\n",
    "    return word_eeg_embedding, EEG_word_level_label\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-30T15:23:55.071460Z",
     "end_time": "2023-11-30T15:23:55.174045Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "#print number of unique words in each task\n",
    "for Task_Dataset in Task_Dataset_List:\n",
    "    subjects = list(Task_Dataset.keys())\n",
    "    print('[INFO]using subjects: ', subjects)\n",
    "    total_num_sentence = len(Task_Dataset[subjects[0]])\n",
    "    print(f'[INFO]total number of sentences = {total_num_sentence}')\n",
    "    unique_words = set()\n",
    "    for key in subjects:\n",
    "        for i in range(total_num_sentence):\n",
    "            if Task_Dataset[key][i] is not None:\n",
    "                sentence_object = Task_Dataset[key][i]\n",
    "                for word in sentence_object['word']:\n",
    "                    unique_words.add(word['content'])\n",
    "    print(f'[INFO]total number of unique words = {len(unique_words)}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-30T15:23:56.070938Z",
     "end_time": "2023-11-30T15:23:58.797474Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "EEG_word_level_embeddings = []\n",
    "EEG_word_level_labels = []\n",
    "#Main loop, looping through each task\n",
    "for Task_Dataset in Task_Dataset_List:\n",
    "    subjects = list(Task_Dataset.keys())\n",
    "    print('[INFO]using subjects: ', subjects)\n",
    "\n",
    "    total_num_sentence = len(Task_Dataset[subjects[0]])\n",
    "\n",
    "    train_divider = int(0.8*total_num_sentence)\n",
    "    dev_divider = train_divider + int(0.1*total_num_sentence)\n",
    "\n",
    "    print(f'train size = {train_divider}')\n",
    "    print(f'dev size = {dev_divider}')\n",
    "\n",
    "\n",
    "    print('[INFO]initializing a train set...')\n",
    "    for key in subjects:\n",
    "        print(f'key = {key}')\n",
    "        for i in range(train_divider):\n",
    "            if Task_Dataset[key][i] is not None:\n",
    "                sentence_object = Task_Dataset[key][i]\n",
    "                for word in sentence_object['word']:\n",
    "                    word_eeg_embedding, EEG_word_level_label = get_eeg_word_embedding(word)\n",
    "                    if word_eeg_embedding is not None and torch.isnan(torch.from_numpy(word_eeg_embedding)).any() == False:\n",
    "                        EEG_word_level_embeddings.append(word_eeg_embedding)\n",
    "                        EEG_word_level_labels.append(EEG_word_level_label)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-30T15:24:36.748135Z",
     "end_time": "2023-11-30T15:24:42.542879Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(EEG_word_level_embeddings)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-30T15:24:47.017870Z",
     "end_time": "2023-11-30T15:24:47.034783Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(EEG_word_level_labels)\n",
    "#count unique items in list\n",
    "unique, counts = np.unique(EEG_word_level_labels, return_counts=True)\n",
    "print(len(unique))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-30T15:24:49.976985Z",
     "end_time": "2023-11-30T15:24:50.082924Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "train_data = []\n",
    "for i in range(len(EEG_word_level_embeddings)):\n",
    "   train_data.append([EEG_word_level_embeddings[i], EEG_word_level_labels[i]])\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-30T14:54:37.339894Z",
     "end_time": "2023-11-30T14:54:37.406224Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-30T15:25:54.041827Z",
     "end_time": "2023-11-30T15:25:54.057741Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Save the lists to a file using pickle\n",
    "with open('EEG_Text_Pairs.pkl', 'wb') as file:\n",
    "    pickle.dump(EEG_word_level_embeddings, file)\n",
    "    pickle.dump(EEG_word_level_labels, file)\n",
    "\n",
    "# To load the lists from the file:\n",
    "with open('EEG_Text_Pairs.pkl', 'rb') as file:\n",
    "    EEG_word_level_embeddings = pickle.load(file)\n",
    "    EEG_word_level_labels = pickle.load(file)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-30T15:25:54.374730Z",
     "end_time": "2023-11-30T15:26:01.195344Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-30T14:54:37.390202Z",
     "end_time": "2023-11-30T14:54:37.422809Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#sant"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-30T14:54:37.406730Z",
     "end_time": "2023-11-30T14:54:37.439372Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-30T14:54:37.422809Z",
     "end_time": "2023-11-30T14:54:37.471930Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
